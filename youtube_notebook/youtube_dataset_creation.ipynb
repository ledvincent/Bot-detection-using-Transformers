{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X3a1QlvEMLR7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data\n",
    "\n",
    "Data can be found here: \"https://www.kaggle.com/datasets/ahsenwaheed/youtube-comments-spam-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = r\"...\\youtube\\Youtube-Spam-Dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Julius NM</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adam riyati</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GsMega</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             AUTHOR                                            CONTENT  CLASS\n",
       "0         Julius NM  Huh, anyway check out this you[tube] channel: ...      1\n",
       "1       adam riyati  Hey guys check out my new channel and our firs...      1\n",
       "2  Evgeny Murashkin             just for test I have to say murdev.com      1\n",
       "3   ElNino Melendez   me shaking my sexy ass on my channel enjoy ^_^ ﻿      1\n",
       "4            GsMega            watch?v=vtaRGgvGtWQ   Check this out .﻿      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(dataset_file, \"r\", encoding='utf-8', errors=\"surrogatepass\") as file:\n",
    "  dataset_youtube = pd.read_csv(file)\n",
    "\n",
    "to_drop = ['COMMENT_ID','DATE','VIDEO_NAME']\n",
    "dataset_youtube.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "dataset_youtube.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7ZEcJPnNajs"
   },
   "source": [
    "This dataset contains users that appear in both categories, so we delete them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bEOiRH1rOqRk"
   },
   "source": [
    "Now we are going to take a random sample of 40000 tweets for training and 10000 tweets for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fu05eLClOool"
   },
   "outputs": [],
   "source": [
    "# Take a random sample of 500000 tweets for training set\n",
    "n = 1600\n",
    "dataset_train = dataset_youtube.sample(n=n)\n",
    "\n",
    "# tweets not in training set\n",
    "dataset_complement = dataset_youtube.loc[dataset_youtube.index.difference(dataset_train.index),:]\n",
    "\n",
    "# take a random sample of 100k in the complementary set\n",
    "dataset_test = dataset_complement.sample(n=dataset_youtube.shape[0]-n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DpP3YPAkQvHn"
   },
   "source": [
    "We can save our training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "buRiLDNsQK1z"
   },
   "outputs": [],
   "source": [
    "dataset_train.to_csv(r'...\\youtube\\dataset\\train.csv')\n",
    "dataset_test.to_csv(r'...\\youtube\\dataset\\test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S21Aymr8SGWg"
   },
   "source": [
    "Now we have generated our training and test datasets"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Generate human and bots tweets dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "m2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
